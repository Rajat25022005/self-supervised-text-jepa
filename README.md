# self-supervised-text-jepa
A research implementation of Joint Embedding Predictive Architectures (JEPA) for text.  Unlike traditional LLMs that predict next tokens, T-JEPA learns semantic representations  by predicting abstract concepts in latent space. Uses momentum encoders (teacher-student)  and operates entirely in embedding space without generative decoding. 
